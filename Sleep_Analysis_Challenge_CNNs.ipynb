{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sleep_Analysis_Challenge_CNNs.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FB6YunD0Gdgc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# \"You Snooze, You Win\" Challenge\n",
        "\n",
        "Every year, the [PhysioNet/CinC (Computing in Cardiology) Challenge](https://www.physionet.org/challenge/) invites \"participants to tackle clinically interesting problems that are either unsolved or not well-solved.\" As you may have figured out from the title, this year's challenge focuses on sleep, particularly the classification of nonarousal and arousal timeframes. If you would like to understand the biological implications of the challenge, we recommend reading PhysioNet's [introduction](https://physionet.org/challenge/2018/) of the challenge.\n",
        "\n",
        "For this challenge, you will classify samples into 5 classes (Arousal, NREM1, NREM2, NREM3, REM). Each sample consists of seven physiological signals (O2-M1, E1-M2, Chin1-Chin2, ABD, CHEST, AIRFLOW, ECG) measured at 200 Hz over a 60 second period (12000 timepoints). In this notebook, we provide code to import the data, visualize sample signals, implement an example classifier, and 'score' your model.\n",
        "\n",
        "### Note:\n",
        "Depending on the model you plan to use, you may want to grab a GPU!"
      ]
    },
    {
      "metadata": {
        "id": "7ctSzKHoGr1q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XxIE6ZwGvjh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "This dataset is a modified version of the PhysioNet/CinC Challenge data, which were contributed by the Massachusetts General Hospital’s Computational Clinical Neurophysiology Laboratory, and the Clinical Data Animation Laboratory.\n",
        "***\n",
        "**Class labels:**\n",
        "- 0 = Arousal\n",
        "- 1 = NREM1\n",
        "- 2 = NREM2\n",
        "- 3 = NREM3\n",
        "- 4 = REM\n",
        "***\n",
        "**Class description:**\n",
        "\n",
        "|Class|State|Characterizations|Brain Waves|Percentage of Sleep|\n",
        "|-|-|-|-|-|-|\n",
        "|Arousal|Consciousness|Wakefulness (coherent cognitive and behavioral responses to external stimuli)|Alpha, Beta|-|\n",
        "|NREM1|Light sleep|Hypnic jerk (involuntary twitch that causes an individual to awaken for a moment)|Theta|5|\n",
        "|NREM2|Unequivocal sleep|Sleep spindle (sudden burst of oscillatory brain activity); K-complex (delta wave that lasts for a second)|Theta, Delta|40-50|\n",
        "|NREM3|Deep sleep|Parasomnias (sleep disorders such as sleepwalking and night terrors)|Delta|15-25|\n",
        "|REM|Dreaming sleep|REM atonia (paralysis of nonessential skeletal muscles); Dreaming|Alpha, Beta|20-25|\n",
        "***\n",
        "**Physiological signal description:**\n",
        "\n",
        "O2-M1 - posterior brain activity (electroencephalography)\n",
        "\n",
        "E1-M2 - left eye activity (electrooculography)\n",
        "\n",
        "Chin1-Chin2 - chin movement (electromyography)\n",
        "\n",
        "ABD - abdominal movement (electromyography)\n",
        "\n",
        "CHEST - chest movement (electromyography)\n",
        "\n",
        "AIRFLOW - respiratory airflow\n",
        "\n",
        "ECG - cardiac activity (electrocardiography)\n",
        "***\n",
        "Run both cell blocks to get the challenge data."
      ]
    },
    {
      "metadata": {
        "id": "yfgbZPNYziXt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# this line will remove the datasets folder if you've already cloned it\n",
        "!rm -rf datasets\n",
        "\n",
        "# and this will clone it again!\n",
        "!git clone https://github.com/BeaverWorksMedlytics/datasets.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k_iKzgvyzioe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "os.chdir('./datasets/Week2_Challenge/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qANF2BvQG2m3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Visualization\n",
        "\n",
        "Run the first cell once to store the training and test sample file locations. If the data have been imported correctly, the cell should output '4000' (training) and '1000' (test).\n",
        "\n",
        "Run the second cell once to initialize important functions that you may find useful. Descriptions of input and output will be provided for every function in the notebook.\n",
        "\n",
        "Run the last cell to visualize a random test sample's seven physiological signals in a raw and FFT (fast fourier transform) format. Note that you can change different parameters, which we will go over in detail.\n",
        "\n",
        "It is important to recognize that the same signal from different samples in the same class may vary in terms of amplitude and frequency. Of course, this is a byproduct of intraspecies variation. Further data preprocessing and/or discriminatory feature extraction may be necessary to account for this phenomenon."
      ]
    },
    {
      "metadata": {
        "id": "-Rw8inOvG5QP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_sample_data(data_type, id_number):\n",
        "  \n",
        "  \"\"\" get the data frame of a single data point by type (training or test) and id (0-999 for test, 0-3999 for training)\"\"\"\n",
        "  \n",
        "  file = './' + data_type + '/' + str(id_number) + '.xz'\n",
        "  sample = pd.read_pickle('./' + file)\n",
        "  return sample, file.split('/')[2]\n",
        "\n",
        "\n",
        "def get_raw_signals(sample):\n",
        "  \n",
        "  \"\"\" get the time (x) and signal (y) for a single data point from the sample dataframe\"\"\"\n",
        "  \n",
        "  raw_signals_x = np.arange(0, 60, step = 1/200)\n",
        "  raw_signals_y = np.transpose(sample['Signal'][0])\n",
        "  return (raw_signals_x, raw_signals_y, 'Raw')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rLlR7m9wx2_Y",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Initalize key reference dictionaries \"\"\"\n",
        "sig_dict = {0:'O2-M1', 1:'E1-M2', 2:'Chin1-Chin2', 3:'ABD', 4:'CHEST', 5:'AIRFLOW', 6:'ECG'}\n",
        "sig_type_dict = {0:'Time (s)', 1:'Frequency (Hz)'}\n",
        "stage_dict = {0:'Arousal', 1:'NREM1', 2:'NREM2', 3:'NREM3', 4:'REM'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pUdLPx8yfrDl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This code can be run for training/validation, as well as the final training and application to test data.\n",
        "* use_test_bool = False --> run training and validation, ignore test data\n",
        "* use_test_bool = True --> re-train algorithm on full training data, apply to test data"
      ]
    },
    {
      "metadata": {
        "id": "NxyTjfv5IFgs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# set to 'False' to run training and validation\n",
        "# set to 'True' to train on full training data and apply to test data\n",
        "use_test_bool = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "idRnai_okHuS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read in the data"
      ]
    },
    {
      "metadata": {
        "id": "Cgg_mNODvjna",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# number of classes we're trying to predict\n",
        "num_classes = 5\n",
        "\n",
        "# extract features from these channels (you may choose not to use them all!)\n",
        "channels = [0,1,2,3,4,5,6]\n",
        "\n",
        "# y labels of training data are in order\n",
        "y_train = np.concatenate([np.zeros(shape=(800,1)),np.ones(shape=(800,1)),2*np.ones(shape=(800,1)),3*np.ones(shape=(800,1)),4*np.ones(shape=(800,1))])\n",
        "\n",
        "# initialize array for holding the raw data\n",
        "X_train = np.ndarray(shape=(4000,12000,len(channels)))\n",
        "\n",
        "\n",
        "# load in the training data\n",
        "for ix_train in range(4000):\n",
        "    samp, fname = get_sample_data('training',ix_train)\n",
        "    t, sig, _ = get_raw_signals(samp)\n",
        "    \n",
        "    X_train[ix_train,:,:] = sig[channels,:].transpose()\n",
        "    \n",
        "    # print out our progress so far...\n",
        "    if ix_train%500 == 0:\n",
        "      print(\"{} of 4000 training data loaded\".format(ix_train))\n",
        "    \n",
        "    \n",
        "# if we're using the test data, load that in here\n",
        "if(use_test_bool):  \n",
        "  X_test = np.ndarray(shape=(1000,12000,len(channels)))\n",
        "  for ix_test in range(1000):\n",
        "      samp, fname = get_sample_data('test',ix_test)\n",
        "      t, sig, type = get_raw_signals(samp)\n",
        "\n",
        "      X_test[ix_test] = sig.transpose()  \n",
        "\n",
        "      if ix_test%200 == 0:\n",
        "        print(\"{} of 1000 testing data loaded\".format(ix_test))\n",
        "\n",
        "print('Done.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ENVtV7ESGBvB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Now that we've read in our data, remove the raw data to free up space\n",
        "!rm -rf datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6BDViT4w0irt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# convert y labels into one-hot vectors\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "\n",
        "\n",
        "# if we're not using the test data, split our training data into train/validation sets\n",
        "if not use_test_bool:\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5fDewRcmkLTC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pre-process our input features\n",
        "Here, we will do a few things:\n",
        "\n",
        "1. Subsample our data so it takes up less memory (we will lose some information)\n",
        "2. Standardize the signals to a unit normal distribution\n",
        "3. Calculate the Fourier transform\n",
        "\n",
        "After these procedures our new input features will be in the frequency domain, not the time domain.  \n",
        "\n",
        "If you want to keep your input in the time domain, simply skip the FFT step."
      ]
    },
    {
      "metadata": {
        "id": "NHU91wNo_sID",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def standardize_data(Xtrain, Xtest=None):\n",
        "  \"\"\"\n",
        "  Standardize our features to unit-normal distribution.\n",
        "  Make sure you standardize using values calculated from Xtrain only!!!\n",
        "  \"\"\"\n",
        "  \n",
        "  # number of channels (we will standardize each separately)\n",
        "  nchannels = Xtrain.shape[2]\n",
        "  \n",
        "  # initialize arrays\n",
        "  Xavg = np.zeros(shape=(nchannels,1))\n",
        "  Xstd = np.zeros(shape=(nchannels,1))\n",
        "  \n",
        "  for channel in range(nchannels):\n",
        "    \n",
        "    # calculate the mean and standard deviation of all the TEST data\n",
        "    Xavg[channel] = X_train[:,:,channel].mean()\n",
        "    Xstd[channel] = X_train[:,:,channel].std()\n",
        "    \n",
        "    \n",
        "    # subtract the mean, divide by standard deviation\n",
        "    Xtrain[:,:,channel] = (X_train[:,:,channel]-Xavg[channel])/Xstd[channel]\n",
        "    \n",
        "    # Make sure we are standardizing based on the mean and std of the training data!\n",
        "    if Xtest is not None:\n",
        "      Xtest[:,:,channel] = (Xtest[:,:,channel]-Xavg[channel])/Xstd[channel]\n",
        "      \n",
        "\n",
        "  return Xtrain, Xtest\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nqA6A878GaI9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def subsample_data(X,k=2):\n",
        "  \"\"\" Sample every k'th data point of the data \"\"\"\n",
        "  \n",
        "  return X[:,range(0,X.shape[1],k),:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fR2vtcGTx-Ps",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_FFT(X):\n",
        "  \"\"\" Calculate the Fourier transform of each data point of X\"\"\"\n",
        "  \n",
        "  npoints = X.shape[0]\n",
        "  N = X.shape[1]\n",
        "  nchannels = X.shape[2]\n",
        "  \n",
        "  # initialize array\n",
        "  X2 =np.zeros(shape=(X.shape[0],X.shape[1]//2,X.shape[2]))\n",
        "  \n",
        "  # for each channel ...\n",
        "  for channel in range(nchannels):\n",
        "    \n",
        "    # for each sample ...\n",
        "    for pt in range(npoints):\n",
        "      \n",
        "      # calculate the Fourier transform\n",
        "      ft = np.fft.fft(X[pt,:,channel])  \n",
        "      \n",
        "      # only store the first 1/2 of the Fourier transform array\n",
        "      X2[pt,:,channel] = np.abs(ft)[:N // 2] * 2 / N\n",
        "    \n",
        "  return X2\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QRWwwYmTGtpo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Subsample the data to reduce the size of our inputs\n",
        "# NOTE: we may lose some of the very high-frequency information, but we may decide that's okay.\n",
        "\n",
        "k=5\n",
        "\n",
        "X_train = subsample_data(X_train,k=k)\n",
        "\n",
        "if use_test_bool:  X_test = subsample_data(X_test,k=k)\n",
        "else:              X_val  = subsample_data(X_val,k=k)\n",
        "\n",
        "# Note that our original data was shape (4000, 12000, n_channels) -- by subsampling we've reduced the size of our inputs\n",
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NrAsxotWaytK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Standardize the data\n",
        "if(use_test_bool): \n",
        "  X_train, X_test = standardize_data(X_train, X_test)\n",
        "  \n",
        "else:\n",
        "  X_train, X_val = standardize_data(X_train, X_val)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pUELfAOUGTPa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Get the FFT of our subsampled, standardized data\n",
        "X_train = get_FFT(X_train)\n",
        "if(use_test_bool): \n",
        "  X_test = get_FFT(X_test)\n",
        "else:\n",
        "  X_val = get_FFT(X_val)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1VGt_bOtzI7z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# After taking the FFT, the size of our data should be cut in half\n",
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rJ4eJwmvk4Qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create our model\n",
        "Here are a few different TensorFlow.Keras models.  Feel free to experiment with different models, some will work better than others for different tasks.  Most of these use 1-D Convolution layers which is particularly appropriate in the time domain."
      ]
    },
    {
      "metadata": {
        "id": "oYcqjqcdckNd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, GlobalMaxPooling1D\n",
        "\n",
        "\n",
        "def make_model(input_shape, modelnum=1):\n",
        "  \n",
        "  # All models are sequential\n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  if modelnum == 1:\n",
        "    \"\"\" Brian's Model (Almost)\"\"\"\n",
        "    \n",
        "    model.add(Conv1D(16, kernel_size=5, activation=tf.nn.relu, input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(pool_size=3, padding='valid'))\n",
        "    model.add(Dropout(0.1))    \n",
        "    model.add(Conv1D(16, kernel_size=5, activation=tf.nn.relu))\n",
        "    model.add(MaxPooling1D(pool_size=3, padding='valid'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Flatten())\n",
        "\n",
        "\n",
        "  elif modelnum == 2:\n",
        "    \"\"\" Two convolutional layers with dropout, should be pretty fast to train \"\"\"\n",
        "    \n",
        "    model.add(Conv1D(32, kernel_size=5, padding=\"same\", activation=tf.nn.relu, input_shape=input_shape)) \n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Conv1D(32, kernel_size=5, padding=\"same\", activation=tf.nn.relu)) \n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(GlobalMaxPooling1D()) \n",
        "    \n",
        "  elif modelnum == 3:\n",
        "    \"\"\" This model is similar to Model #2 but with different kernel sizes\"\"\"\n",
        "    \n",
        "    model.add(Conv1D(32, kernel_size=15, activation=tf.nn.relu, input_shape=input_shape)) \n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Conv1D(32, kernel_size=15, activation=tf.nn.relu)) \n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(GlobalMaxPooling1D()) \n",
        "    \n",
        "  elif modelnum == 4:\n",
        "    \"\"\" Also similar to Model #2\"\"\"\n",
        "    \n",
        "    model.add(Conv1D(64, kernel_size=9, activation=tf.nn.relu, input_shape=input_shape)) \n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv1D(32, kernel_size=9, activation=tf.nn.relu)) \n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(GlobalMaxPooling1D()) \n",
        "    \n",
        "    \n",
        "  elif modelnum == 5:\n",
        "    \"\"\" Single convolution layer model\"\"\"\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=20, strides=1, padding=\"same\",  activation = tf.nn.relu, input_shape=input_shape))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    \n",
        "  elif modelnum == 6:\n",
        "    \"\"\" This model is similar to Model #4 but with slightly different parameters\"\"\"\n",
        "    \n",
        "    model.add(Conv1D(32, kernel_size=5, padding=\"valid\", activation=tf.nn.relu, input_shape=input_shape)) \n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Conv1D(32, kernel_size=5, padding=\"valid\", activation=tf.nn.relu)) \n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(GlobalMaxPooling1D()) \n",
        "\n",
        "  elif modelnum == 7:\n",
        "    \"\"\" This model doesn't use convolutions at all!\"\"\"\n",
        "    \n",
        "    model.add(MaxPooling1D(pool_size=15, padding='valid', input_shape=input_shape))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation=tf.nn.relu))\n",
        "\n",
        "\n",
        "  else:\n",
        "    return None\n",
        "  \n",
        "  # All models end with a dense layer with softmax activation\n",
        "  model.add(Dense(num_classes, activation=tf.nn.softmax))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DN5zAIHm33PY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Choose which model you want to train\n",
        "\n",
        "model = make_model(input_shape=X_train[0].shape, modelnum=1)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRbsCUJ1mARJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compile and train the model"
      ]
    },
    {
      "metadata": {
        "id": "d4o2INYOafHI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.train.AdamOptimizer(0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uZx1Lx_Uor3S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size=200\n",
        "epochs=100\n",
        "\n",
        "if use_test_bool:\n",
        "  history = model.fit(X_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              verbose=2)\n",
        "else:\n",
        "  history = model.fit(X_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(X_val, y_val),\n",
        "              verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t_5ug8vTmJFn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plot the Training (and Validation) Accuracy over training epochs"
      ]
    },
    {
      "metadata": {
        "id": "_Mt-8Etk8zeL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(history.history['acc'],label='training')\n",
        "if not use_test_bool:\n",
        "  plt.plot(history.history['val_acc'],label='validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'],label='training')\n",
        "if not use_test_bool:\n",
        "  plt.plot(history.history['val_loss'],label='validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (Cross-Entropy)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LDQWbhUK6cMt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model\n",
        "\n",
        "- **Area Under the ROC Curve (AUCROC)**: The receiver operating characteristic (ROC) curve plots the true positive rate (sensitivity/recall) against the false positive rate (fall-out) at many decision threshold settings. The area under the curve (AUC) measures discrimination, the classifier's ability to correctly identify samples from the \"positive\" and \"negative\" cases. Intuitively, AUC is the probability that a randomly chosen \"positive\" sample will be labeled as \"more positive\" than a randomly chosen \"negative\" sample. In the case of a multi-class ROC curve, each class is considered separately before taking the weighted average of all the class results. Simply put, the class under consideration is labeled as \"positive\" while all other classes are labeled as \"negative.\" Below is the multi-class ROC curve for the example classifier. The AUCROC score should be between 0 and 1, in which 0.5 is random classification and 1 is perfect classification.\n",
        "\n",
        " ![alt text](https://image.ibb.co/g4pzST/ROCAUC.png)\n",
        "\n",
        "- **Matthews Correlation Coefficient (MCC)**: The MCC measures the quality of binary classifications, irrespective of the class sizes. Importantly, it is typically regarded as a balanced measure since it considers all values in the 2x2 contingency table (TP, FP, TN, FN). For this challenge, the binary classes will be \"Arousal\" (Arousal) and \"Nonarousal\" (NREM1, NREM2, NREM3, REM). The MCC score should be between -1 and 1, in which 0 is random classification and 1 is perfect classification.\n",
        "\n",
        " ![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/5caa90fc15105b74b59a30bbc9cc2e5bd43a13b7)\n",
        "\n",
        "Using these metrics, the example classifier has the following scores on test data:\n",
        "- AUCROC: 0.755\n",
        "- MCC: 0.353\n",
        "- Creativity: ( ͡° ͜ʖ ͡°)\n",
        "\n",
        "\n",
        "Below is the code used to calculate the AUCROC and MCC metrics when evaluating your classifier (shown evaluated on the validation data)"
      ]
    },
    {
      "metadata": {
        "id": "2T470EHWsg8I",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# ROC AUC on validation data\n",
        "\n",
        "if not use_test_bool:\n",
        "  \n",
        "  y_val_pred = model.predict(X_val)\n",
        "  y_val_pred = pd.DataFrame(y_val_pred)\n",
        "  y_val = pd.DataFrame(y_val)\n",
        "  \n",
        "  fpr = {}\n",
        "  tpr = {}\n",
        "  roc_auc = {}\n",
        "  \n",
        "  plt.figure(figsize=(8,8))\n",
        "  for i in range(num_classes):\n",
        "      fpr[i], tpr[i], _ = metrics.roc_curve(y_val.iloc[:, i], y_val_pred.iloc[:, i])\n",
        "      roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
        "      plt.plot(fpr[i], tpr[i], label = stage_dict[i] + ', ' + str(i), lw=3)   \n",
        "  \n",
        "  plt.plot([0, 1], [0, 1],':')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Multi-Class ROC Curve')\n",
        "  plt.legend(fontsize='large')\n",
        "  plt.show()\n",
        "\n",
        "  fpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(y_val.values.ravel(), y_val_pred.values.ravel())\n",
        "  roc_auc = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "  print('ROC AUC: {:0.3f}'.format(roc_auc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U16kzuSVum65",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# MCC on validation data\n",
        "\n",
        "if not use_test_bool:\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "\n",
        "  test_true = y_val.idxmax(axis=1)\n",
        "  test_predict = y_val_pred.idxmax(axis=1)\n",
        "\n",
        "  for i in range(y_val_pred.shape[0]):\n",
        "\n",
        "      if test_predict.iloc[i]==0: y_pred.append(1)\n",
        "      else: y_pred.append(-1)\n",
        "\n",
        "      if test_true[i]==0: y_true.append(1)\n",
        "      else: y_true.append(-1)\n",
        "\n",
        "  mcc = metrics.matthews_corrcoef(y_true, y_pred)\n",
        "  print('MCC: {:0.3f}'.format(mcc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hjMIsks4mPoY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## If you're applying to test data, use the model you've just trained to predict test labels"
      ]
    },
    {
      "metadata": {
        "id": "D1C6Cz1mculz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if use_test_bool:\n",
        "\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  y_test_pred = pd.DataFrame(y_test_pred)\n",
        "  \n",
        "  # Take a peek, make sure the output makes sense\n",
        "  print(y_test_pred.head(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BPp9FbdSmi8O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Write predictions to file"
      ]
    },
    {
      "metadata": {
        "id": "A7Jgh2Zdcuqv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "filename = 'My_Test_Predictions.xz'\n",
        "y_test_pred.to_pickle(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "joLQ_cOrmuuP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To download the pickle file to your computer, this line should work:"
      ]
    },
    {
      "metadata": {
        "id": "tQcp0e_mmu54",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "files.download(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KtxgSkZhm051",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If it doesn't work (it doesn't work for me...) you will need to give Colab permission to write the file to your Google Drive, and then you can retrieve the file from there."
      ]
    },
    {
      "metadata": {
        "id": "_bgkKBEwwIGW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vuX5aP-ewWRq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Save the pickle file to your Google Drive\n",
        "file = drive.CreateFile()\n",
        "file.SetContentFile(filename)\n",
        "file.Upload() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "muRVTnFKnPxP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save your TensorFlow model\n",
        "I had trouble getting Colab to cooperate, but perhaps you will have better luck!"
      ]
    },
    {
      "metadata": {
        "id": "FUJ2mPb3vN_J",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "tf.keras.models.save_model(\n",
        "    model,'my_tf_model')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}